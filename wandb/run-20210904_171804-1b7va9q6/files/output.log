
Random Sampling
Training parameters: {'LOG_DIR': '../Proxy_Anchor/Newlogs', 'dataset': 'cub', 'sz_embedding': 512, 'sz_batch': 90, 'nb_epochs': 40, 'gpu_id': 1, 'nb_workers': 2, 'model': 'bn_inception', 'loss': 'Proxy_Anchor', 'optimizer': 'adamw', 'lr': 0.0001, 'mrg_lr': 0.0005, 'weight_decay': 0.0001, 'weight_lambda': 0.3, 'lr_decay_step': 10, 'lr_decay_gamma': 0.5, 'mrg_lr_decay_step': 10, 'mrg_lr_decay_gamma': 0.5, 'alphap': 48.0, 'alphan': 48.0, 'mrg': 0.1, 'IPC': None, 'warm': 1, 'bn_freeze': 1, 'l2_norm': 1, 'remark': '', 'delta': 0.1, 'T': 1.0, 'lam': 1.0}
Training for 40 epochs.







Train Epoch: 0 [63/65 (95%)] Loss: 14.863398: : 63it [00:14,  4.34it/s]

Train Epoch: 0 [65/65 (98%)] Loss: 14.744200: : 65it [00:15,  4.21it/s]
/media/wyf/CUB_200_2011/images/101.White_Pelican/White_Pelican_0003_96691.jpg
Computing t-SNE embedding
0it [00:00, ?it/s]
R@1 : 50.456
R@2 : 63.825
R@4 : 76.199
R@8 : 85.213
R@16 : 91.847
R@32 : 95.949







Train Epoch: 1 [60/65 (91%)] Loss: 12.603323: : 60it [00:14,  4.38it/s]

Train Epoch: 1 [65/65 (98%)] Loss: 13.335241: : 65it [00:16,  3.99it/s]
/media/wyf/CUB_200_2011/images/101.White_Pelican/White_Pelican_0003_96691.jpg
Computing t-SNE embedding
0it [00:00, ?it/s]
Train Epoch: 2 [4/65 (5%)] Loss: 12.329308: : 4it [00:01,  3.51it/s]
R@1 : 60.635
R@2 : 71.826
R@4 : 81.246
R@8 : 88.201
R@16 : 93.501








Train Epoch: 2 [65/65 (98%)] Loss: 11.161931: : 65it [00:15,  4.17it/s]
**Evaluating...**
/media/wyf/CUB_200_2011/images/101.White_Pelican/White_Pelican_0003_96691.jpg
Computing t-SNE embedding
0it [00:00, ?it/s]
R@1 : 62.930
Train Epoch: 3 [4/65 (5%)] Loss: 10.333913: : 4it [00:01,  3.47it/s]
R@2 : 74.173
R@4 : 83.660
R@8 : 89.905
R@16 : 94.362








Train Epoch: 3 [65/65 (98%)] Loss: 9.552786: : 65it [00:15,  4.15it/s]
**Evaluating...**
/media/wyf/CUB_200_2011/images/101.White_Pelican/White_Pelican_0003_96691.jpg
Computing t-SNE embedding
0it [00:00, ?it/s]
Train Epoch: 4 [4/65 (5%)] Loss: 9.159286: : 4it [00:01,  3.45it/s]
R@1 : 64.095
R@2 : 75.473
R@4 : 83.710
R@8 : 90.294
R@16 : 94.801








Train Epoch: 4 [65/65 (98%)] Loss: 9.276093: : 65it [00:15,  4.16it/s]
**Evaluating...**
/media/wyf/CUB_200_2011/images/101.White_Pelican/White_Pelican_0003_96691.jpg
Computing t-SNE embedding
0it [00:00, ?it/s]
R@1 : 66.307
R@2 : 76.789
R@4 : 85.111
R@8 : 91.020
R@16 : 94.919
R@32 : 97.248







Train Epoch: 5 [61/65 (92%)] Loss: 9.393817: : 61it [00:14,  4.25it/s]

Train Epoch: 5 [65/65 (98%)] Loss: 8.372562: : 65it [00:15,  4.13it/s]
/media/wyf/CUB_200_2011/images/101.White_Pelican/White_Pelican_0003_96691.jpg
Computing t-SNE embedding
0it [00:00, ?it/s]
Train Epoch: 6 [1/65 (0%)] Loss: 8.397634: : 1it [00:00,  1.23it/s]
R@1 : 64.669
R@2 : 76.165
R@4 : 84.369
R@8 : 89.703
R@16 : 94.159








Train Epoch: 6 [65/65 (98%)] Loss: 7.892289: : 65it [00:15,  4.10it/s]
**Evaluating...**
/media/wyf/CUB_200_2011/images/101.White_Pelican/White_Pelican_0003_96691.jpg
Computing t-SNE embedding
0it [00:00, ?it/s]
R@1 : 66.458
R@2 : 77.887
R@4 : 85.618
R@8 : 91.577
R@16 : 94.970
R@32 : 97.181






Train Epoch: 7 [59/65 (89%)] Loss: 7.586675: : 59it [00:15,  3.87it/s]
Traceback (most recent call last):
  File "train.py", line 505, in <module>
    main()
  File "train.py", line 442, in main
    opt.step()
  File "/home/wyf/anaconda3/envs/proxyanchor/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/wyf/anaconda3/envs/proxyanchor/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in wrapper
    return func(*args, **kwargs)
  File "/home/wyf/anaconda3/envs/proxyanchor/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/wyf/anaconda3/envs/proxyanchor/lib/python3.8/site-packages/torch/optim/adamw.py", line 104, in step
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
KeyboardInterrupt